# Pipeline AWS SQS para S3

[![Python](https://img.shields.io/badge/Python-3.12-blue.svg)](https://python.org)
[![AWS](https://img.shields.io/badge/AWS-SQS%20|%20S3-orange.svg)](https://aws.amazon.com)
[![Docker](https://img.shields.io/badge/Docker-Enabled-blue.svg)](https://docker.com)
[![Poetry](https://img.shields.io/badge/Poetry-Dependency%20Management-green.svg)](https://python-poetry.org)

> ğŸ“– **DocumentaÃ§Ã£o**: [PortuguÃªs](#pipeline-aws-sqs-para-s3) | [English](#aws-sqs-to-s3-pipeline-english)

Um pipeline que demonstra prÃ¡ticas modernas de engenharia de dados usando serviÃ§os AWS. Este projeto gera dados sintÃ©ticos de usuÃ¡rios, processa-os atravÃ©s de filas Amazon SQS e armazena os resultados em buckets Amazon S3.

## VisÃ£o Geral da Arquitetura

```
[Gerador de Dados] â†’ [Amazon SQS] â†’ [Consumidor/Processador] â†’ [Amazon S3]
```

![DescriÃ§Ã£o da imagem](media/architecture.png)


1. **GeraÃ§Ã£o de Dados**: Cria dados sintÃ©ticos de usuÃ¡rios usando a biblioteca Faker
2. **Fila de Mensagens**: Envia dados para AWS SQS para processamento confiÃ¡vel de mensagens
3. **Processamento do Consumidor**: Recupera mensagens, transforma dados para formato JSON
4. **Armazenamento**: Faz upload de arquivos processados para AWS S3 com limpeza automÃ¡tica

## ğŸ“‹ PrÃ©-requisitos

### Requisitos AWS
- Conta AWS com permissÃµes apropriadas
- UsuÃ¡rio IAM com permissÃµes de acesso ao SQS e S3
- AWS Access Key ID e Secret Access Key

### Desenvolvimento Local
- Python 3.12+
- Poetry (para gerenciamento de dependÃªncias)
- Docker (opcional, para execuÃ§Ã£o containerizada)

## âš™ï¸ InstalaÃ§Ã£o

### ConfiguraÃ§Ã£o

Altere o nome do arquivo `.env-example` para `.env` e insira suas credenciais AWS

```env
AWS_ACCESS_KEY_ID=<seu_aws_access_key_id_aqui>
AWS_SECRET_KEY=<sua_aws_secret_key_aqui>
REGION_NAME=<sua_aws_region_name_aqui>
```

### OpÃ§Ã£o 1: ConfiguraÃ§Ã£o Local com Poetry

1. **Clone o repositÃ³rio**
   ```bash
   git clone git@github.com:vgrcontreras/aws-sqs-to-s3-etl.git
   cd aws-sqs-to-s3-etl
   ```

2. **Instale as dependÃªncias**
   ```bash
   pip install poetry
   poetry install
   ```

3. **Execute a aplicaÃ§Ã£o**
   ```bash
   poetry run python -m src.main
   ```

### OpÃ§Ã£o 2: ConfiguraÃ§Ã£o com Docker

1. **Construa a imagem Docker**
   ```bash
   docker build -t aws-sqs-to-s3-etl .
   ```

2. **Execute com arquivo de ambiente**
   ```bash
   docker run --env-file .env aws-sqs-to-s3-etl
   ```

## ğŸ“ Estrutura do Projeto

```
aws-sqs-to-s3-etl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Ponto de entrada principal da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ producer.py          # Produtor de mensagens SQS
â”‚   â”œâ”€â”€ consumer.py          # Consumidor de mensagens SQS e uploader S3
â”‚   â”œâ”€â”€ generate_users.py    # GeraÃ§Ã£o de dados sintÃ©ticos
â”‚   â”œâ”€â”€ create_queue.py      # CriaÃ§Ã£o de fila SQS
â”‚   â””â”€â”€ create_s3_bucket.py  # CriaÃ§Ã£o de bucket S3
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ aws_client.py        # ConfiguraÃ§Ã£o do cliente AWS
â”‚   â””â”€â”€ settings.py          # ConfiguraÃ§Ãµes de ambiente
â”œâ”€â”€ Dockerfile               # ConfiguraÃ§Ã£o do container
â”œâ”€â”€ pyproject.toml          # DependÃªncias Poetry
â””â”€â”€ README.md               # DocumentaÃ§Ã£o do projeto
```

## ğŸ” Monitoramento e Logging

A aplicaÃ§Ã£o usa Loguru para logging abrangente:

- **NÃ­vel Info**: OperaÃ§Ãµes bem-sucedidas, criaÃ§Ã£o de filas, uploads de arquivos
- **NÃ­vel Error**: Erros de serviÃ§os AWS, falhas em operaÃ§Ãµes de arquivos
- **NÃ­vel Debug**: InformaÃ§Ãµes detalhadas de processamento

Exemplo de saÃ­da de log:
```
2024-01-15 10:30:15 | INFO | Fila criada com sucesso. QueueUrl: https://sqs.us-east-1.amazonaws.com/123456789012/aws-sqs-to-s3-etl-queue
2024-01-15 10:30:16 | INFO | Mensagem enviada para a fila com sucesso.
2024-01-15 10:30:17 | INFO | users_20240115_103017.json enviado para aws-sqs-to-s3-etl-bucket com sucesso
```

---

# AWS SQS to S3 Pipeline (English)

> ğŸ“– **Documentation**: [PortuguÃªs](#pipeline-aws-sqs-para-s3) | [English](#aws-sqs-to-s3-pipeline-english)

A pipeline that demonstrates modern data engineering practices using AWS services. This project generates synthetic user data, processes it through Amazon SQS queues, and stores the results in Amazon S3 buckets.

## Architecture Overview

```
[Data Generator] â†’ [Amazon SQS] â†’ [Consumer/Processor] â†’ [Amazon S3]
```

![DescriÃ§Ã£o da imagem](media/architecture.png)

1. **Data Generation**: Creates synthetic user data using Faker library
2. **Message Queue**: Sends data to AWS SQS for reliable message processing
3. **Consumer Processing**: Retrieves messages, transforms data to JSON format
4. **Storage**: Uploads processed files to AWS S3 with automatic cleanup

## ğŸ“‹ Prerequisites

### AWS Requirements
- AWS Account with appropriate permissions
- IAM user with SQS and S3 access permissions
- AWS Access Key ID and Secret Access Key

### Local Development
- Python 3.12+
- Poetry (for dependency management)
- Docker (optional, for containerized execution)

## âš™ï¸ Installation

### Configuration

Change `.env-example` file name to `.env` and insert your AWS credentials

```env
AWS_ACCESS_KEY_ID=<your_aws_access_key_id_here>
AWS_SECRET_KEY=<your_aws_secret_key_here>
REGION_NAME=<your_aws_region_name_here>
```

### Option 1: Local Setup with Poetry

1. **Clone the repository**
   ```bash
   git clone git@github.com:vgrcontreras/aws-sqs-to-s3-etl.git
   cd aws-sqs-to-s3-etl
   ```

2. **Install dependencies**
   ```bash
   pip install poetry
   poetry install
   ```

3. **Run the application**
   ```bash
   poetry run python -m src.main
   ```

### Option 2: Docker Setup

1. **Build the Docker image**
   ```bash
   docker build -t aws-sqs-to-s3-etl .
   ```

2. **Run with environment file**
   ```bash
   docker run --env-file .env aws-sqs-to-s3-etl
   ```

## ğŸ“ Project Structure

```
aws-sqs-to-s3-etl/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Main application entry point
â”‚   â”œâ”€â”€ producer.py          # SQS message producer
â”‚   â”œâ”€â”€ consumer.py          # SQS message consumer and S3 uploader
â”‚   â”œâ”€â”€ generate_users.py    # Synthetic data generation
â”‚   â”œâ”€â”€ create_queue.py      # SQS queue creation
â”‚   â””â”€â”€ create_s3_bucket.py  # S3 bucket creation
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ aws_client.py        # AWS client configuration
â”‚   â””â”€â”€ settings.py          # Environment settings
â”œâ”€â”€ Dockerfile               # Container configuration
â”œâ”€â”€ pyproject.toml          # Poetry dependencies
â””â”€â”€ README.md               # Project documentation
```

## ğŸ” Monitoring and Logging

The application uses Loguru for comprehensive logging:

- **Info Level**: Successful operations, queue creation, file uploads
- **Error Level**: AWS service errors, file operation failures
- **Debug Level**: Detailed processing information

Example log output:
```
2024-01-15 10:30:15 | INFO | Queue create successfully. QueueUrl: https://sqs.us-east-1.amazonaws.com/123456789012/aws-sqs-to-s3-etl-queue
2024-01-15 10:30:16 | INFO | Message send to queue successfully.
2024-01-15 10:30:17 | INFO | users_20240115_103017.json uploaded to aws-sqs-to-s3-etl-bucket successfully
```

